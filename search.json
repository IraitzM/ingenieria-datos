[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ingeniería de datos",
    "section": "",
    "text": "Prólogo\nSeamos honestos, el mundo de la ingeniería de datos es un lío. Tenemos que lidiar con el desarrollo de las aplicaciones que recaban datos, fuentes internas, externas, con distintos formatos y cadencias de datos,… Y unificar todo en un repositorio que permita un buen gobierno de los datos, facilite su uso para analistas y científicos de datos, permita una evolución de nuestro modelo de datos corporativo y sea fácil e intuitivo.\nAh, y barato a poder ser. Todo un reto.\nNuestro posición como ingenieros de datos está en el epicentro de la acción dada la relevancia que han tomado los campos de la ciencia de datos e inteligencia artificial. Además, el ámbito corporativo necesita de ciertas garantías para poder dar cumplimiento al marco regulatorio cuando se trata de datos de carácter personal o con impacto a usuarios.\nPor ello debemos de datos buena cuenta de las mejores prácticas a la hora de realizar nuestra tarea.",
    "crumbs": [
      "Prólogo"
    ]
  },
  {
    "objectID": "index.html#referencias",
    "href": "index.html#referencias",
    "title": "Ingeniería de datos",
    "section": "Referencias",
    "text": "Referencias\nTenéis muy buenos recursos online, aunque en inglés muchos:\n\nEl roadmap ideal lleno de recursos de pago y gratuitos\nLa wiki del ingeniero de datos\nMuchos recursos muy buenos por Zach Wilson de DataExpert.io\nData Engineering por Karl Christian\nEl gran curso de Joe Reis en Deeplearning.ai\n\nAhora toca digerir todo esto. Con suerte esta guía os puede asistir.",
    "crumbs": [
      "Prólogo"
    ]
  },
  {
    "objectID": "content/intro.html",
    "href": "content/intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Inicios\nEste libro pretende ser una guía sencilla con referencias al estado de la cuestión y herramientas en boga. Hablaremos de:\nE intentaremos cubrir las últimas tendencias en herramientas, en la medida de lo posible. Para ello, deberemos hacernos eco del ciclo de vida de los datos en toda su extensión. Aunque nos centraremos mucho en los conceptos y las herramientas serán ejemplos en los que podemos implementar estas ideas.\nEl del ingeniero de datos es un rol a caballo entre las áreas de negocio y los desarrolladores de software, tomando gran parte de las prácticas aprendidas de estos. Aunque enfocándonos en la puesta al servicio de los datos.\nY empezaremos con un concepto que no está exento de controversia: el modelado de datos. Aunque para ello, debemos entender el contexto histórico que rodea a los ingenieros de datos trasladándonos a los primeros sistemas de gestión de datos y su aparición a finales de los años 60 del siglo pasado.\nCon frecuencia olvidamos el recorrido que hemos realizado en el mundo de la informática y cómo impacta las decisiones o aspectos de los sistemas de hoy día.\nPor eso es importante entender el trasfondo histórico y las herencias que a pesar de no ser muy lógicas, determinan algunas de las prácticas que nos encontramos en la actualidad.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "content/intro.html#inicios",
    "href": "content/intro.html#inicios",
    "title": "1  Introducción",
    "section": "",
    "text": "Curiosidades\n\n\n\n\n\nEs habitual que las nuevas generaciones no sepan el origen del símbolo de guardar.\nEl disquete que era un medio físico para guardar contenido (el pendrive de los antiguos).\n\n\n\nDisquete físico\n\n\nPero mucha gente se olvida que la configuración de teclado que empleamos en la actualidad es España, QWERTY por las primeras teclas en el extremo superior izquierdo de los teclados, es una herencia de las máquinas de escribir y que esta configuración a pesar de no ser la que nos permite una mayor velocidad de redacción impedía que los martillos de la máquina se bloquearan.\n\n\n\nMartillos en una máquina de escribir\n\n\nLa misma razón por la que la tecla enter en sus inicios se conocía como retorno de carro (return carriage) cuando no existe carro que retornar en nuestros ordenadores modernos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "content/intro.html#los-sistemas-centrales",
    "href": "content/intro.html#los-sistemas-centrales",
    "title": "1  Introducción",
    "section": "1.2 Los sistemas centrales",
    "text": "1.2 Los sistemas centrales\nEn sus inicios, las máquinas evolucionaron de máquinas individuales donde tarjetas perforadas determinaban los automatismos, a sistemas electrónicos y sofisticados que disponían de canales de entrada y salida (teclados y pantallas) para realizar las acciones que determinaban las acciones de la máquina.\n\n\n\nAquellos maravillosos años\n\n\nInternational Business Machines (IBM) fue una de las primeras compañías en realizar esa transición de máquinas que habían servido para fines militares al ámbito comercial, corporativo, al servicio de instituciones privadas con problemas de grandes dimensiones.\nEn aquella época las máquinas que hoy llamamos ordenadores o computadoras eran recursos muy costosos y por ello, existían unidades centrales a las que se accedía desde nodos terminales. Sistemas únicamente de acceso a la máquina a la que estaban conectadas para pedirle que hiciera algo, introducir datos de cuentas de cliente, operaciones de cálculo y balances, etc. Lo habitual en la empresa donde este rol lo cubrían secretarias que venían de ámbitos como la mecanografía.\nNo existía internet además y una vez salíamos del dominio de la empresa no se podía actuar sobre esos sistemas. Había que hablar en un lenguaje de muy bajo nivel con la máquina, poco intuitivo para gente poco versada en esas tecnologías. Eran tiempos oscuros.\nEstos sistemas gestionaban tanto los datos que los empleados iban registrando así como las acciones que se realizaban sobre estos con rutinas que reflejan la operativa de la compañía: procesos de compra, transferencias bancarias, facturación de servicios y contabilidad general, entre otros.\nSon los que conocemos como sistemas centrales de datos y gran parte de este trabajo se lo debemos a una persona en concreto, Edgar Frank Codd, y su trabajo en este ámbito titulado Un modelo tabular1 de datos para grandes bancos de datos compartidos (Codd 1970).\n\n\n\n\n\n\nMainframe y COBOL\n\n\n\n\n\nOs sorprendería la cantidad de empresas que aún a día de hoy emplean máquinas mainframe con programas COBOL que muy pocos individuos a nivel global saben a ciencia cierta lo que hacen. Es parte de la deuda técnica que muchas empresas no terminan de quitarse de encima por mucho avance de la tecnología que haya. Es parte de nuestra componente humana y la dificultad que nos supone el cambio.\n\n\n\n\n\n\n\nCodd, Edgar F. 1970. «A relational model of data for large shared data banks». Communications of the ACM 13 (6): 377-87.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "content/intro.html#footnotes",
    "href": "content/intro.html#footnotes",
    "title": "1  Introducción",
    "section": "",
    "text": "Uso la traducción tabular de el término inglés relation que se refiere a la estructura de los datos y la traducción a relacional puede dar a entender una gestión relacional que no es tal en los sistemas que ideó Codd.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "content/modelado/intro.html",
    "href": "content/modelado/intro.html",
    "title": "Modelado de datos",
    "section": "",
    "text": "Entidades y relaciones\nDebemos considerar que el modelado de datos era algo obligatorio en los sistemas iniciales. Las bases de relacionales requerían informar del esquema de sus estructuras de datos antes de poder insertar datos en estas. Es decir, debo informar de los campos y su tipología (texto, valores numéricos, etc.) antes de poder informar de los datos asociados.\nEn la década de los 2000 esto fue alterado debido al crecimiento de sistemas flexibles en ese sentido, las bases de datos NoSQL y sistemas Hadoop que no requerían informar de las estructuras de datos almacenadas. Pero es una cuestión de quién realiza esta tarea, no tanto de no necesitarla más, ya que la estructuración de los datos es algo obligatorio para poder entender lo que tenemos entre manos.\nHaremos un pequeño barrido histórico y así quizás podamos entender las opciones actuales. Pero primero es crucial que entendamos que el modelado es el medio por el que conseguimos trasladar nuestra realidad a un medio técnico.\nEl proceso de modelar los datos de una empresa es en primera instancia un entendimiento de sus procesos productivos. Debemos entender como interactúan las distintas partes para así conocer los datos que estos procesos generaran y las necesidades de estos.\nSolemos tener la urgencia de correr a las implementaciones técnicas pero estas pueden variar dependiendo las necesidades del sistema que manejemos. Sin embargo, el modelado lógico solo cambiará cuando los procesos de negocio cambien o nuestra empresa incluya nuevas líneas de negocio.\nDe este modo, el modelado es un proceso que parte de la estructura más abstracta hasta la implementación técnica del modelo.",
    "crumbs": [
      "Modelado de datos"
    ]
  },
  {
    "objectID": "content/modelado/intro.html#entidades-y-relaciones",
    "href": "content/modelado/intro.html#entidades-y-relaciones",
    "title": "Modelado de datos",
    "section": "",
    "text": "Entidades\nUna entidad en un diagrama entidad-relación representa un objeto o concepto del mundo real (persona, lugar o cosa ) que puede ser identificado de manera única y sobre el cual se desea almacenar información. nos referimos a estos conceptos como conceptos nucleares de negocio o Core Business Concept (CBC)1. Una entidad que contiene distintos atributos que describen características de esta.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n\n\n\n\n\n\nPodríamos tener varias entidades que representan conceptos clave (personas, lugares, unidades corporativas, etc.) de nuestros procesos de negocio. Si tuviéramos una academia, por ejemplo, resulta claro que existirán entidades como estas y muy posiblemente querremos recolectar información con respecto a cada una. Esto determina los atributos que deberemos declarar.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n\n    ASIGNATURAS {\n        numero id_asignatura\n        texto nombre\n    }\n\n    PROFESORES {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n\n\n\n\n\n\n\n\nRelaciones\nLa clave es entender cómo interactúan entre sí. Por ejemplo, un alumno estará matriculado a una o varias asignaturas. Y un profesor imparte una o varias asignaturas.\n\n\n\n\n\nerDiagram\n    ALUMNOS ||--|| ASIGNATURAS : matriculado\n    PROFESORES ||--|| ASIGNATURAS : imparte\n\n\n\n\n\n\nA estas nociones, (matriculado e imparte) las conocemos como relaciones (Natural Business Relationships2) que no son más que eventos clave de nuestra operativa y juntas conforman un diagrama entidad relación, que representa de forma lógica el qué y cómo opera nuestra empresa de una forma lógica. El hecho de transformar e ir concretando el modelo conceptual a esta representación más detallada se la conoce como modelado lógico, donde deberemos ir definiendo aspectos clave a la hora de poder explotar esta información y responder preguntas que surgirán sobre los datos.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n\n    ASIGNATURAS {\n        numero id_asignatura\n        texto nombre\n    }\n\n    PROFESORES {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n\n    ALUMNOS ||--|| ASIGNATURAS : matriculado\n    PROFESORES ||--|| ASIGNATURAS : imparte\n\n\n\n\n\n\nEstas relaciones también pueden disponer de atributos, como el año de matriculación o impartición, y esto define la granularidad de estas relaciones. Es decir, cómo se relacionan las instancias de las distintas entidades entre sí. Veamos un ejemplo sencillo:\nun PROFESOR IMPARTIÓ en Febrero del 2025 una ASIGNATURA \nVemos cómo las entidades clave (CBC) y sus relaciones naturales son definidas con artículos que enumeran la relación un ... una....\nun EMPLEADO VENDIÓ 500 unidades de un PRODUCTO a un CLIENTE\nun CLIENTE TRANSFIRIÓ x cantidad de euros a otro CLIENTE\nNos ayuda a entender el contexto de información que deberá albergar nuestro sistema. Por ejemplo,\nun PROFESOR IMPARTE varias ASIGNATURAs por CURSO/AÑO.\nun ALUMNO RECIBE varias ASIGNATURAs por CURSO/AÑO.\npero\nun ALUMNO debe tener un único TUTOR por CURSO/AÑO.\nGran parte de este primer ejercicio es poder aclarar estos aspectos clave:\n\nEntidades clave (CBC)\nRelaciones entre entidades (NBR)\nAtributos\nCardinalidad de las relaciones\n\nUna vez este esquema está claro y consolidado con nuestros interlocutores las unidades de negocio podemos empezar a aterrizar en base a las necesidades técnicas qué forma final tomarán nuestros datos en su repositorio final.\n\n\n\nProceso de modelado de datos\n\n\nEste proceso debe siempre contar con la validación de los responsables de negocio y la mejor manera es poder poner ejemplos de cosas que el sistema pueda o no albergar.\n\nnosotros: Un ejemplo de lo que no podremos guardar un ALUMNO puede tener varios TUTORes por CURSO/AÑO.\nnegocio: Aunque no debería, un año nos pasó porque un tutor se dió de baja durante el año lectivo.\nnosotros: Ok, incluiremos una opción para que pueda suceder que un alumno tenga varios tutores en un mismo curso, con atributos de hasta en la relación de ALUMNO y TUTOR.\n\nResulta que allá por los años en los que todo encajaba en tablas, existían varios problemas a la hora de gestionar estas estructuras de datos de forma que resultara un esquema consistente y con buen rendimiento. Para resolver problemas de los sistemas de la época se definieron las formas normales.",
    "crumbs": [
      "Modelado de datos"
    ]
  },
  {
    "objectID": "content/modelado/intro.html#footnotes",
    "href": "content/modelado/intro.html#footnotes",
    "title": "Modelado de datos",
    "section": "",
    "text": "Muy recomendables los artefactos en ELM Standards al respecto de los CBC: CBC list↩︎\nDe la misma fuente, sobre los NBR: NBR matrix↩︎",
    "crumbs": [
      "Modelado de datos"
    ]
  },
  {
    "objectID": "content/modelado/formasnormales.html",
    "href": "content/modelado/formasnormales.html",
    "title": "2  Formas normales",
    "section": "",
    "text": "2.1 Primera forma normal (1FN)\nCuando Edgar Codd diseño los sistemas de gestión de datos tabulares o RDBMS (Relational Data Base Management Systems) su planteamiento no cubría únicamente la necesidad de informar el esquema, si no de que este cubriera las necesidades de los procesos de negocio. De manera que estos sistemas fueran los garantes de que la base de datos cumpliera estas restricciones y siempre ofreciera una visión consistente de la información.\nUn sencillo ejemplo es pensar en una estructura de datos que almacena nuestro balance en cuenta. Si nuestro banco no permite que ese balance sea negativo, ninguna acción debería permitir que esto suceda a nivel del sistema que los almacena y gestiona. Codd se aseguró de diseñar un sistema que ofreciera esas capacidades a modo de restricción. Del mismo modo, no debería existir más de una persona con el mismo identificador único… De ahí es de donde nace el concepto de clave primaria que permite identificar unívocamente un registro en nuestra tabla.\nLas formas normales nacen precisamente de los trabajos de Codd como solución a algunos de los problemas que presentaban los sistemas iniciales. El planteamiento inicial es que los datos deben ser almacenados en tablas, con lo que podemos representar de forma visual nuestras tablas del siguiente modo.\nEjemplo de una tabla sobre alumnos apuntados a asignaturas\nSolemos representar estas estructuras indicando la información de su esquema:\nEsto genera una ilustración de entidad como esta. Una entidad que contiene distintos atributos que en el contexto tabular conocemos como columnas de la tabla.\nLa primera forma normal (1FN) establece que todos los valores de los atributos en una tabla deben ser atómicos, es decir, cada campo debe contener un solo valor y no listas o conjuntos. Esto implica eliminar grupos repetitivos y asegurar que cada columna almacene únicamente un dato por fila.\nEsto es una limitación de los sistemas de aquella época que los actuales no disponen, pero bien es cierto que si la información corresponde a una entidad relevante como son las asignaturas de una academia, con multiples interacción y datos relativos a distintas operaciones, bien se antoja que dispongan de su propia entidad como veremos más adelante.\nVentajas de la 1FN:\nEjemplo de una tabla en 1FN sobre alumnos apuntados a asignaturas\nDisponer de la información de este modo nos obliga a que cualquier cambio sobre los datos de los alumnos deba realizarse en varias filas. Resulta difícil ver qué concepto identifica la clave primaria en estos casos. ¿Un alumno en una asignatura? ¿Es la asignatura Matemáticas solo cursada por Ana?\nEn nuestro caso, el nombre del campo id_alumno refiere a cada alumno pero esta información está por duplicado y no permite identificar filas concretas. Debemos separar conceptos, por un lado tenemos ALUMNOS y por el otro ASIGNATURAS.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formas normales</span>"
    ]
  },
  {
    "objectID": "content/modelado/formasnormales.html#primera-forma-normal-1fn",
    "href": "content/modelado/formasnormales.html#primera-forma-normal-1fn",
    "title": "2  Formas normales",
    "section": "",
    "text": "Facilita la consulta y manipulación de los datos.\nEvita la ambigüedad y redundancia en los registros.\nPermite una mejor organización y estructuración de la información.\n\n\n\n\nid_alumno\nnombre\napellido\nasignatura\n\n\n\n\n001\nAna\nGarcía\nMatemáticas\n\n\n001\nAna\nGarcía\nFísica\n\n\n002\nLuis\nPérez\nQuímica\n\n\n002\nLuis\nPérez\nBiología\n\n\n003\nMarta\nLópez\nHistoria\n\n\n003\nMarta\nLópez\nInglés",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formas normales</span>"
    ]
  },
  {
    "objectID": "content/modelado/formasnormales.html#segunda-forma-normal-2fn",
    "href": "content/modelado/formasnormales.html#segunda-forma-normal-2fn",
    "title": "2  Formas normales",
    "section": "2.2 Segunda forma normal (2FN)",
    "text": "2.2 Segunda forma normal (2FN)\nLa segunda forma normal (2FN) se alcanza cuando una tabla está en primera forma normal y, además, todos los atributos que no forman parte de la clave principal dependen completamente de ella, no solo de una parte de la clave si esta es compuesta. Existen situaciones donde un solo atributo no es suficiente para identificar unívocamente cada instancia. Por ejemplo, si realizamos una orden de venta, donde tenemos varios productos, la cantidad de producto que se vendió en cada orden de compra viene determinada por al dupla, ORDEN y PRODUCTO, solo conociendo a qué ORDEN y PRODUCTO nos referimos podemos determinar un valor de forma única de cantidad, descuento u otros conceptos asociados a ese caso particular.\n\n\n\nid_venta\nid_producto\ncantidad\n\n\n\n\n001\n001\n5\n\n\n001\n002\n10\n\n\n002\n001\n7\n\n\n\nEjemplo de evento de venta con cantidad de producto\nEsto implica eliminar dependencias parciales, dividiendo la información en varias tablas si es necesario.\nVentajas de la 2FN:\n\nReduce la redundancia de datos.\nFacilita la actualización y mantenimiento de la base de datos.\nEvita inconsistencias al modificar información relacionada.\n\nLa segunda forma normal nos obliga a separar nuestra tabla en dos entidades separadas: alumnos y asignaturas. Esto tiene sentido ya que si somos una escuela, serán dos entidades relevantes que se relacionarán con otras entidades de forma individualizada. Por ejemplo, si añadiéramos una tercera entidad, profesores, esta tendrá relaciones específicas y concretas con las asignaturas y no con los alumnos directamente a priori.\nPara indicar las relaciones entre entidades solemos indicar conexiones entrantes y salientes de los dos esquemas.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n    ASIGNATURAS {\n        numero id_asignatura\n        texto nombre\n    }\n\n\n\n\n\n\nLas relaciones varían dependiendo de la cardinalidad de las mismas. Es decir, como se relacionan los datos entre sí:\n\n1-a-1: Una asignatura tiene solo un alumno y viceversa\n1-a-muchos: Una asignatura tiene muchos alumnos, pero cada alumno solo tiene una asignatura\nmuchos-a-muchos: Las asignaturas tiene muchos alumnos y cada alumno puede estar en varias asignaturas.\n\nEntrada wikipedia\nEn nuestro ejemplo anterior la relación entre alumnos y asignaturas es muchos-a-muchos, lo cual requiere una tabla intermedia para que la relación entre ambas entidades no genere un volumen excesivo de datos al desnormalizar la información.\n\n\n\n\n\n\nDesnormalizar\n\n\n\nSi bien es cierto que la normalización de los datos en estos términos nos permite una mejor gestión, casi seguro que el consumo de estos requerirá construir tablas donde se combine la información de alumnos, profesores y asignaturas, como bien pudiera ser un informe de carga de trabajo por profesor o de alumnos promedio por aula. Casi siempre la información la consumiremos de una forma distinta a la forzada para ser almacenada y gestionada.\n\n\nPara indicar el tipo de relación se emplean indicaciones visuales como el hecho de que la relación tenga como mínimo una relación existente (con el circulo se marca que pueda no existir dato asociado), o si la relación identifica a a las partes.\nSi queremos indicar que un alumno puede o no estar matriculado en muchas asignaturas, mientras que una asignatura debe tener solo un alumno (es una academia muy privada) lo indicaremos así:\n\n\n\n\n\nerDiagram\n    ALUMNOS ||--o{ ASIGNATURAS: matriculado\n\n\n\n\n\n\nMientras que si entendemos que para formar parte de la academia las asignaturas debe tener al menos un alumno (y pueden tener muchos) y cada alumno deberá tener al menos una asignatura (y puede matricularse en varias), lo haremos así:\n\n\n\n\n\nerDiagram\n    ALUMNOS }|--|{ ASIGNATURAS: matriculado",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formas normales</span>"
    ]
  },
  {
    "objectID": "content/modelado/formasnormales.html#tercera-forma-normal-3fn",
    "href": "content/modelado/formasnormales.html#tercera-forma-normal-3fn",
    "title": "2  Formas normales",
    "section": "2.3 Tercera forma normal (3FN)",
    "text": "2.3 Tercera forma normal (3FN)\nLa tercera forma normal (3FN) se alcanza cuando una tabla está en segunda forma normal y, además, todos los atributos que no forman parte de la clave principal no dependen transitivamente de ella. Es decir, cada campo debe depender únicamente de la clave primaria y no de otros atributos no clave.\nEsto resuelve problemas como la redundancia y las dependencias indirectas, evitando que la modificación de un dato implique cambios en múltiples lugares y reduciendo el riesgo de inconsistencias. La 3FN facilita el mantenimiento y la integridad de la base de datos, asegurando que cada dato se almacene en un solo lugar y que las relaciones entre entidades sean claras y directas.\nPor ejemplo, bien pudiera ser que un mismo profesor imparta varias asignaturas, con lo que el identificador de la asignatura no identifica univocamente a nuestros profesores.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n    ASIGNATURAS {\n        numero id_asignatura\n        texto nombre\n        texto profesor\n    }\n\n\n\n\n\n\nEsto merece separar este concepto en una entidad independiente con su propio identificador.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n    ASIGNATURAS {\n        numero id_asignatura\n        texto nombre\n    }\n    PROFESORES {\n        numero id_profesor\n        texto nombre\n    }\n\n\n\n\n\n\nY posteriormente podremos unir las relaciones entre las entidades:\n\nUn alumno debe estar matriculado en al menos una asignatura\nUna asignatura debe tener al menos un alumno matriculado para ser cursada\nUn profesor puede dar multiples asignatura con la opción de no estar impartiendo ninguna\nUna asignatura solo puede ser impartida por un profesor\n\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n    }\n    ASIGNATURAS {\n        numero id_asignatura\n        texto nombre\n    }\n    PROFESORES {\n        numero id_profesor\n        texto nombre\n    }\n    ALUMNOS }|--|{ ASIGNATURAS: matriculado\n    PROFESORES ||--o{ ASIGNATURAS: imparte\n\n\n\n\n\n\nCon este esquema claro podemos ver como debemos disponer la información para que nuestro sistema gestor de datos pueda contener la estructura tal y como necesitamos.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formas normales</span>"
    ]
  },
  {
    "objectID": "content/modelado/desnormalizar.html",
    "href": "content/modelado/desnormalizar.html",
    "title": "3  Desnormalizar",
    "section": "",
    "text": "3.1 Relación 1-a-1/n\nComo comentábamos anteriormente, aunque debamos normalizar nuestra estructura informativa, en muchos casos deberemos volver a juntarla en una sola respuesta para poder informar de todos los campos de interés para nuestra consulta.\nPor ejemplo ¿qué profesor tiene una carga mayor de alumnos? Pues para ello deberemos juntar la información almacenada en las entidades y relaciones que nos permiten llegar de un extremo (PROFESORES) al otro (ALUMNOS) a través de las relaciones y entidades intermedias.\nDe forma que obtengamos como resultado…\nEjemplo de resultado con información agregada\nEn este ejemplo debemos relacionar los profesores con sus asignaturas impartidas y los alumnos que las cursan para así poder contabilizar, por profesor cuantos tienen. Si solo informamos del nombre puede ser que existan varios Javier, pero cada uno tendrá una relación bien definida con respecto a las asignaturas y alumnos asociados gracias a su clave primaria.\nDentro de las distintas formas en las que podemos asociar dos entidades, las de 1-a-1 o 1-a-n quizás resulten las más sencillas, dado que únicamente requieren que seamos capaces de añadir un campo que haga referencia a una instancia concreta de una entidad. Y la forma más sencilla de hacer esto es mediante la referencia a su clave primaria. Cuando esta clave es empleada de forma referencial, nos referiremos a ellas como claves foráneas.\nPor ejemplo, asumamos que un alumno solo cursa una asignatura. Podemos incluir el campo de la asignatura que cursa como atributo en su entidad e identificarla como clave foránea (FK) mientras que el mismo campo en la tabla ASIGNATURAS será clave primaria (PK). Sucedería igual si cada asignatura es impartida por un único docente.\nerDiagram\n    ALUMNOS {\n        numero id_alumno\n        texto nombre\n        texto apellido\n        numero id_asignatura FK\n    }\n    ASIGNATURAS {\n        numero id_asignatura PK\n        texto nombre\n        numero id_docente FK\n    }\n    PROFESORES {\n        numero id_profesor PK\n        texto nombre\n    }\n    ALUMNOS }|--|{ ASIGNATURAS: matriculado\n    PROFESORES ||--o{ ASIGNATURAS: imparte",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desnormalizar</span>"
    ]
  },
  {
    "objectID": "content/modelado/desnormalizar.html#relación-n-a-m",
    "href": "content/modelado/desnormalizar.html#relación-n-a-m",
    "title": "3  Desnormalizar",
    "section": "3.2 Relación n-a-m",
    "text": "3.2 Relación n-a-m\nCuando la relación es de muchos a muchos, las bases de datos tabulares (RDBMS) presentan un problema, que es el poder conjugar esta información de forma que no explote. Multiplicar todas las filas de ua entidad relacionada con la otra y a su vez que estas presenten todas las de la primera resulta en un producto cartesiano difícil de gestionar de una sola vez.\n\n\n\n\n\n\nProducto cartesiano\n\n\n\n\n\nEn bases de datos relacionales, el producto cartesiano ocurre cuando combinamos dos tablas sin especificar una condición de relación entre ellas. Esto genera todas las combinaciones posibles de filas entre ambas tablas, lo que puede resultar en un número de registros extremadamente grande y difícil de manejar.\nPor ejemplo, si tenemos una tabla de 10 alumnos y otra de 5 asignaturas, el producto cartesiano generaría 10 x 5 = 50 filas, donde cada alumno aparece emparejado con cada asignatura, aunque no tenga relación real entre ellos. Esto no solo consume recursos innecesarios, sino que también puede llevar a resultados incorrectos si no se filtra adecuadamente la información relevante mediante condiciones de unión (JOIN) apropiadas.\n\n\n\nPara evitar esta problemática, las relaciones se guardan en una tabla que registra estas relaciones como filas con dos claves foráneas de las entidades asociadas que forma clave principal para potenciales atributos que sean particulares de esa relación.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno PK\n        texto nombre\n        texto apellido\n    }\n    CURSA{\n        numero id_alumno FK\n        numero id_asignatura FK\n        numero anio\n    }\n    ASIGNATURAS {\n        numero id_asignatura PK\n        texto nombre\n        numero id_docente FK\n    }\n    PROFESORES {\n        numero id_profesor PK\n        texto nombre\n    }\n\n    ALUMNOS }|--|{ CURSA: matriculado\n    CURSA }|--|{ ASIGNATURAS: cursando\n    PROFESORES ||--o{ ASIGNATURAS: imparte\n\n\n\n\n\n\nEsto nos permitiría por ejemplo, variar el esquema también si existe la posibilidad de que una asignatura en un año se imparta por uno o varios profesores a distintos alumnos.\n\n\n\n\n\nerDiagram\n    ALUMNOS {\n        numero id_alumno PK\n        texto nombre\n        texto apellido\n    }\n    CURSA{\n        numero id_alumno FK\n        numero id_asignatura FK\n        numero id_profesor FK\n        numero anio\n    }\n    ASIGNATURAS {\n        numero id_asignatura PK\n        texto nombre\n    }\n    PROFESORES {\n        numero id_profesor PK\n        texto nombre\n    }\n\n    ALUMNOS }|--|{ CURSA: matriculado\n    CURSA }|--|{ ASIGNATURAS: cursando\n    PROFESORES ||--o{ CURSA: imparte\n\n\n\n\n\n\nEs cuestión de entender bien el contexto ya que una vez determinamos las restricciones efectivas mediante estas relaciones, no podremos por ejemplo introducir un profesor no registrado en nuestro sistema como docente (error de clave foránea), eliminar un alumno de nuestros registros si está cursando aún alguna asignatura, etc… Es clave definir bien estos conceptos para luego operar sobre los datos y más aún, saber qué tablas es necesario combinar para obtener la información necesaria.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desnormalizar</span>"
    ]
  },
  {
    "objectID": "content/modelado/desnormalizar.html#joins",
    "href": "content/modelado/desnormalizar.html#joins",
    "title": "3  Desnormalizar",
    "section": "3.3 JOINs",
    "text": "3.3 JOINs\nLas famosas JOINs no son más que la especificación técnica de cómo queremos juntar los datos para poder extraer la información necesaria. Si queremos conocer todos los alumnos que tiene un profesor, deberemos sobre el esquema anterior juntar las tablas CURSA, ALUMNOS y PROFESORES (al menos) para poder recurrir a sus atributos. Esto es lo que nos obliga a generar consultas algo más complejas ya que debemos realizar esa desnormalización donde las tablas vuelven a tomar una forma similar a la original.\n\n\n\n\n\n\n\n\n\n\n\nid_alumno\nnombre\napellido\nasignatura\nnombre\nid_profesor\n\n\n\n\n001\nAna\nGarcía\nMatemáticas\nJavier\n001\n\n\n001\nAna\nGarcía\nFísica\nJesús\n002\n\n\n002\nLuis\nPérez\nQuímica\nJulian\n003\n\n\n002\nLuis\nPérez\nBiología\nJavier\n001\n\n\n003\nMarta\nLópez\nHistoria\nJavier\n004\n\n\n003\nMarta\nLópez\nInglés\nJason\n005\n\n\n\nEjemplo de una tabla en desnormalizada\nEn caso de que dispongamos de coincidencias en los nombres de los campos (¿nombre se refiere al atributo en PROFESORES o en ALUMNOS?) que en estos casos formarán parte de la misma tabla usarmos el concepto de alias o renombrado para indicar un numbro que haga referencia al campo de forma más clara.\n\n\n\n\n\n\n\n\n\n\n\nid_alumno\nnombre_alumno\napellido\nasignatura\nnombre_profe\nid_profesor\n\n\n\n\n001\nAna\nGarcía\nMatemáticas\nJavier\n001\n\n\n001\nAna\nGarcía\nFísica\nJesús\n002\n\n\n002\nLuis\nPérez\nQuímica\nJulian\n003\n\n\n002\nLuis\nPérez\nBiología\nJavier\n001\n\n\n003\nMarta\nLópez\nHistoria\nJavier\n004\n\n\n003\nMarta\nLópez\nInglés\nJason\n005\n\n\n\nEjemplo de una tabla en desnormalizada con alias\nDebemos entender que la normalización se presenta como un mecanismo de gestión de la información que no atiende tanto al consumo posterior si no a la consistencia de los datos con un rendimiento base, de modo que pueda servir a su propósito al margen de los consumos posteriores que puedan ser necesarios y las composiciones de los datos en esos nuevos escenarios. Esto nos obliga a considerar el uso que haremos de la información existiendo dos caracteres muy diferenciados en toda organización en lo que a sistemas se refiere.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desnormalizar</span>"
    ]
  },
  {
    "objectID": "content/modelado/modelado_analitico.html",
    "href": "content/modelado/modelado_analitico.html",
    "title": "4  Modelado analítico",
    "section": "",
    "text": "4.1 Modelado dimensional\nEl modelado cubre una necesidad muy real y operativa. El modelado, tal y como lo hemos vista hasta ahora permite eliminar redundancias, hacer la gestión de datos coherente con los requisitos del negocio y aún así, sacar el máximo provecho del sistema. Sin embargo no está orientado a ser sencillo de operar o entender por parte de aquellos que quieren extraer conocimiento de los datos.\nPor ello, ya que de todos modos tenemos que disponer un sistema pensado para analítica, por qué no darle una forma que haga la información más fácil de consumir. Aunque este proceso no está falto de trabajo. William (Bill) Inmon es reconocido como el padre del concepto almacén de datos o data warehouse. Un data warehouse es esencialmente ese repositorio donde residen todos los datos de la compañía, no de forma operacional, si no para poder realizar consultas, análisis e informes sobre ellos. Esto permite además unificar las distintas fuentes para dar una visión cohesionada de los procesos en la empresa y sus datos asociados.\nSin embargo, el modelo de Inmon requería una consolidación entre fuentes que es difícil de construir. Además, el ritmo al que operan las distintas unidades hace que existan urgencias de datos que “no pueden esperar”. Un enfoque algo más pragmático que permitía enfocarse en necesidades concretas es asociado con Ralph Kimball. Fue años más tarde y quizás el progreso de la tecnología hacia los 90 hizo posible el enfoque de Kimball pero sin duda es uno de los modelados más comunes cuando nos acercamos a las áreas de consumo de datos. De hecho, el término Business Intelligence se inicia en esta época gracias al foco en el warehousing y el modelado analítico.\nEl modelado dimensional, en estrella o también referido Kimball, se enfoca en disponer las dimensiones de corte de los análisis en tablas muy próximas a los conceptos de entidad que comentamos en el modelado Entidad-Relación y los hechos, acontecimientos en otra entidad de crecimiento más frecuente y vertical.\nerDiagram\n    DIM_EMPLEADOS {\n        numero id_empleado PK\n        texto nombre\n        texto apellido\n        texto desc_territorio\n    }\n    DIM_CLIENTES {\n        numero id_cliente PK\n        texto nombre\n    }\n    DIM_PRODUCTOS {\n        numero id_producto PK\n        texto nombre\n        texto categoria\n        numero precio_unitario\n        texto proveedor\n    }\n    DIM_TIEMPO {\n        numero id_tiempo PK\n        numero anio\n        numero mes\n        numero trimestre\n        texto nombre_mes\n        numero dia\n        texto dia_semana\n    }\n    FACT_FACTURAS {\n        numero id_tiempo FK\n        numero id_cliente FK\n        numero id_producto FK\n        numero id_empleado FK\n        numero total\n    }\n\n    DIM_TIEMPO }|--o{ FACT_FACTURAS: fecha\n    DIM_PRODUCTOS }|--o{ FACT_FACTURAS: producto\n    DIM_CLIENTES }|--o{ FACT_FACTURAS: cliente\n    DIM_EMPLEADOS }|--o{ FACT_FACTURAS: empleado\nEl enfoque se basa en que las dimensiones son instancias de datos poco cambiantes (listado de empleados, clientes, productos,…) mientras que los hechos crecen de forma constante (en nuestro caso las ventas facturadas). Aunque esto no siempre pasa así y podemos tener información cambiante en nuestras dimensiones como veremos más adelante.\nLas dimensiones cubren aspectos relativos a puntos de corte en nuestros análisis:\nMientras que los hechos son los hechos cuantificables:\nDe modo que la pregunta de negocio\nSe aterriza indicando:\nPudiendo variar únicamente la métrica (cuantas unidades, cuantos dolares, qué porcentaje de descuentos, …). Esto hace que muchas consultas puedan componerse de forma aditiva en base a preguntas frecuentes, atributos concretos usados frecuentemente, agregadas de forma distinta.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelado analítico</span>"
    ]
  },
  {
    "objectID": "content/modelado/modelado_analitico.html#modelado-dimensional",
    "href": "content/modelado/modelado_analitico.html#modelado-dimensional",
    "title": "4  Modelado analítico",
    "section": "",
    "text": "quién\nqué\ndónde\ncómo\n\n\n\ncuántos\nen total\npromedio\n\n\n\n¿cuantos pedidos que contengan salsa de tomate hemos tenido en este último trimestre?\n\n\n\nDIM_PRODUCTOS: salsa de tomate\nDIM_TIEMPO: último trimestre\nFACT_FACTURAS: número de facturas asociadas a DIMs\n\n\n\n4.1.1 SCD\nEl concepto de Slowly Changing Dimension (SCD) nos obliga a indicar distintos valores para nuestras dimensiones en base a una condición temporal. Un producto que cambia de nombre a una fecha dada, un cliente que cambia de apellidos,… Existen varias modalidades con las que podemos intentar paliar este hecho, y una de las más comunes es historificar los cambios con fechas de efecto, también conocido como dimensión de tipo 2.\n\n\n\n\n\nerDiagram\n    DIM_EMPLEADOS {\n        numero id_empleado PK\n        texto nombre\n        texto apellido\n        texto desc_territorio\n        fecha efectivo_desde\n        fecha efectivo_hasta\n    }\n    DIM_CLIENTES {\n        numero id_cliente PK\n        texto nombre\n        fecha efectivo_desde\n        fecha efectivo_hasta\n    }\n\n\n\n\n\n\nEsto nos obliga a añadir condiciones que filtren los registros vigentes, o indiquen la fecha de efecto pudiendo así consultar la información como hubiera sido consultada en un intervalo entre fechas efectivas. Esto sin duda complica la composición de consulta, aunque nos permite copar con estos cambios.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelado analítico</span>"
    ]
  },
  {
    "objectID": "content/modelado/modelado_analitico.html#modelado-de-copo-de-nieve",
    "href": "content/modelado/modelado_analitico.html#modelado-de-copo-de-nieve",
    "title": "4  Modelado analítico",
    "section": "4.2 Modelado de copo de nieve",
    "text": "4.2 Modelado de copo de nieve\nEl modelo de copo de nieve extiende el modelo estrella arriba mencionado a jerarquías de dimensiones. Pensemos que por ejemplo, cuando hablamos del territorio que cubre un empleado, esto se refiere a una entidad o dimensión en esta etapa, donde se determinan todos los territorios existentes.\n\n\n\n\n\nerDiagram\n    DIM_TERRITORIOS {\n        numero id_territorio PK\n        texto desc_territorio\n    }\n    DIM_EMPLEADOS {\n        numero id_empleado PK\n        texto nombre\n        texto apellido\n        texto id_territorio FK\n    }\n    DIM_CLIENTES {\n        numero id_cliente PK\n        texto nombre\n    }\n    DIM_PROVEEDORES {\n        numero id_proveedor PK\n        texto nombre\n        texto direccion\n    }\n    DIM_CATEGORIAS {\n        numero id_categoria PK\n        texto categoria\n    }\n    DIM_PRODUCTOS {\n        numero id_producto PK\n        texto nombre\n        numero precio_unitario\n        numero id_categoria FK\n        numero id_proveedor FK\n    }\n    DIM_TRIMESTRES {\n        numero id_trimestre PK\n        texto trimestre\n        numero num_trimestre\n    }\n    DIM_MESES {\n        numero id_mes PK\n        texto mes\n        numero num_mes\n    }\n    DIM_TIEMPO {\n        numero id_tiempo PK\n        numero anio\n        numero id_mes FK\n        numero id_trimestre FK\n    }\n    FACT_FACTURAS {\n        numero id_tiempo FK\n        numero id_cliente FK\n        numero id_producto FK\n        numero id_empleado FK\n        numero total\n    }\n\n    DIM_TERRITORIOS }|--o{ DIM_EMPLEADOS: en\n    DIM_CATEGORIAS }|--o{ DIM_PRODUCTOS: pertenece\n    DIM_PROVEEDORES }|--o{ DIM_PRODUCTOS: provee\n\n    DIM_TRIMESTRES }|--o{ DIM_TIEMPO: en\n    DIM_MESES }|--o{ DIM_TIEMPO: en\n\n    DIM_TIEMPO }|--o{ FACT_FACTURAS: fecha\n    DIM_PRODUCTOS }|--o{ FACT_FACTURAS: producto\n    DIM_CLIENTES }|--o{ FACT_FACTURAS: cliente\n    DIM_EMPLEADOS }|--o{ FACT_FACTURAS: empleado\n\n\n\n\n\n\nSi conseguimos tener dimensiones comunes a todas las áreas de negocio, esto nos da una consolidación de la información con poca variación entre unidades. Sin embargo, suele ser común la variación entre unidades de esas dimensiones comunes a nuestra actividad. El control sobre estas dimensiones clave ha hecho que la gestión de datos maestros se convierta en una actividad en si misma.\n\n4.2.1 Gestión de datos maestros\nLa gestión de datos maestros (Master Data Management, MDM) es una disciplina tecnológica en la que las empresas y los departamentos de TI colaboran para garantizar la uniformidad, la precisión, la gestión, la coherencia semántica y la responsabilidad de los activos de datos maestros compartidos de la empresa.\nEs un marco utilizado por empresas que buscan aprovechar sus datos de manera más efectiva, asegurando la precisión, coherencia y accesibilidad de los datos comerciales centrales, como información de clientes, detalles de productos o registros de proveedores.\nMDM se define como una disciplina en la que el negocio y la tecnología trabajan juntos para asegurar la uniformidad, la exactitud, la custodia, la consistencia semántica y la responsabilidad de los activos de datos maestros oficiales de la empresa.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelado analítico</span>"
    ]
  },
  {
    "objectID": "content/modelado/modelado_analitico.html#data-marts",
    "href": "content/modelado/modelado_analitico.html#data-marts",
    "title": "4  Modelado analítico",
    "section": "4.3 Data Marts",
    "text": "4.3 Data Marts\nSiendo prácticos, por mucho que dispongamos de toda la información de una empresa, solo necesitaremos ciertas dimensiones y hechos para conformar nuestras métricas y realizar nuestro análisis. Habitualmente la organización se dispone de manera que un subconjunto de los datos es accesible por la unidad que los necesita y a ese subconjunto lo conocemos como data mart. Dónde reside y cómo se modela no cambia, pero es el nombre que se le atribuye a esas colecciones de tablas que permiten a la unidad gestionar los datos necesarios en el universo de datos disponibles.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelado analítico</span>"
    ]
  },
  {
    "objectID": "content/modelado/nuevos_paradigmas.html",
    "href": "content/modelado/nuevos_paradigmas.html",
    "title": "5  Nuevos paradigmas",
    "section": "",
    "text": "5.1 Modelos lógicos ensamblados\nPrecisamente conciliar estas necesidades de consumo por un lado, y de fuentes de datos cambiantes por el otro hace que disponer una pieza central, modelada y coherente con todos los procesos de la empresa de forma histórica resulte excepcionalmente retador. De hecho, los conceptos que determinaban el concepto Data Warehouse definido por Bill Inmon requerían:\nEsto nos obliga a guardar todas las versiones temporales en una forma que permita tanto la integración de nuevas fuentes como el consumo orientado a dominio en un sistema centralizado.\nEnsemble Logical Modelling (ELM) es una técnica de abstracción que nos permite realizar una integración de los datos enfocándonos en las entidades de negocio (CBC) y sus relaciones naturales (NBR) de forma que todo lo demás una vez esta estructura base existe es la asociación de datos a estos conceptos.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nuevos paradigmas</span>"
    ]
  },
  {
    "objectID": "content/modelado/nuevos_paradigmas.html#modelos-lógicos-ensamblados",
    "href": "content/modelado/nuevos_paradigmas.html#modelos-lógicos-ensamblados",
    "title": "5  Nuevos paradigmas",
    "section": "",
    "text": "5.1.1 Data Vault\nUna de las implementaciones lógicas de las ELM se la debemos a Daniel Lindstedt. Su enfoque, llamado Data Vault, realiza un mapeo bastante fiel de nuestro modelo lógico, con una flexibilidad adicional sobre la información ligada a los conceptos clave y las relaciones naturales:\n\nHUBs: Donde se registran las instancias de las entidades\nLINKs: Donde se registran los eventos o hechos que vinculan una o más entidades\nSATELITEs: Donde se alberga la información asociada a una entidad para una fecha concreta\n\n\n\n\nEstructuras base de un Data Vault (HUBs, LINKs y SATELITEs)\n\n\nTanto los HUBs como los LINKs llevan huellas temporales de carácter efectivo e identifican la fuente de donde la instancia fue informada, de manera que resulta sencillo realizar viajes temporales a los datos efectivos a una fecha dada. El hecho de trabajar con huellas temporales hace que podamos plantear un sistema en el que simplemente nuevos datos son añadidos y en tiempo de consumo se seleccionen aquellos que tomen efecto.\nEn un mecanismo intermediario entre datos de fuentes normalizadas y entornos donde el consumo se realiza en base a dimensiones y hechos. Algunos lo regieren incluso como una metodología ágil debido a sus características. Son un perfecto nexo que permite una adición incremental pero con huella temporal de todos los hechos acontecidos y permite promocionar la información consumible a las capas de acceso de los usuarios en base a sus necesidades.\nExisten otras modalidades que extienden más aún este concepto aunque Data Vault ha sido quizás el paradigma que más adopción presenta teniendo en cuenta la complejidad técnica y madurez del equipo que requiere su correcto gobierno. Cabe destacar que todos estos conceptos no implican un uso de tablas para almacenar la información. Esto ha venido determinado por la tecnología disponible en cada momento.",
    "crumbs": [
      "Modelado de datos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nuevos paradigmas</span>"
    ]
  },
  {
    "objectID": "content/sistemas/intro.html",
    "href": "content/sistemas/intro.html",
    "title": "6  Sistemas de gestión de datos",
    "section": "",
    "text": "6.1 Operacional\nLos sistemas de gestión de datos deben cumplir el requisito de responder en tiempo y forma a las consultas que se les plantean, aunque estas pueden venir de dos propósitos muy diferenciados.\nLos sistemas pensados para el procesamiento de transacciones en línea (On Line Transaction Processing o OLTP) y los sistemas para el procesamiento analítico en línea (On Line Analytical Processing o OLAP) son dos enfoques diferentes para el manejo de bases de datos, diseñados para propósitos distintos dentro de la gestión de datos y el propio ciclo de vida que estos cumplen.\nOLTP se centra en el procesamiento de transacciones en tiempo real, como operaciones bancarias, compras en línea o gestión de inventarios, y está optimizado para manejar un gran número de transacciones cortas que implican operaciones de inserción, actualización y eliminación (INSERT, UPDATE, DELETE). Estos sistemas priorizan tiempos de respuesta rápidos (en milisegundos), alta concurrencia y consistencia de datos mediante propiedades ACID, lo que garantiza la integridad de las transacciones.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sistemas de gestión de datos</span>"
    ]
  },
  {
    "objectID": "content/sistemas/intro.html#analítico",
    "href": "content/sistemas/intro.html#analítico",
    "title": "6  Sistemas de gestión de datos",
    "section": "6.2 Analítico",
    "text": "6.2 Analítico\nEn contraste, los sistemas OLAP están diseñados para el análisis de grandes volúmenes de datos históricos y agregaciones voluminosas, apoyando tareas de inteligencia de negocio como informes, planificación y descubrimiento de tendencias. Las consultas OLAP suelen ser complejas, implican agregaciones (como suma, promediado) y análisis multidimensional, y pueden tardar desde segundos hasta horas en ejecutarse. Estos sistemas están optimizados para operaciones de lectura masiva y utilizan estructuras particulars como el esquema en estrella o en copo de nieve para mejorar el rendimiento de las consultas analíticas.\nPrecisamente por ser los sistemas transaccionales la fuente de información para las plataformas que soportan consultas OLAP, que requieren de reestructurar parte de la información, una pieza clave en este ecosistema son los procesos de transformación de datos.\n\n6.2.1 ETL/ELT\nExtract, Load y Transform son las siglas con los que se asocian los sistemas destinados a extraer, transformar y cargar la información entre distintos sistemas gestores de datos. Pueden darse en distinto orden en base a si la información a transformar puede ser gestionada en un sistema intermedio entre origen y destino (ETL) o si se precisa de volumen de información y capacidad de computo, de forma que podamos usar el sistema destino como motor de transformación (ELT).\nEsto nos permite dibujar un ciclo de vida esperado de los datos que requiere nuestra intervención en distintos puntos y con distintas necesidades.\n\n\n\nCiclo de vida de los datos\n\n\nQuizás esta foto es un buen ejemplo de por qué los ingenieros de datos deben emplear:\n\nOrquestadores para coordinar las distintas fases\nPlataformas de computación distribuida\nSistemas de validación y registro de metadatos para resolver problemas mediante linaje de datos\nProgramación, desde SQL y simple Bash a lenguajes más flexibles como Python o frameworks como Apache Spark, Kafka Streams, etc…\n\nLos ingenieros tenemos cierta tendencia en centrarnos en las herramientas pero no debemos olvidar que el propósito es disponer los datos en tiempo informa para nuestros clientes, los usuarios de los datos.Veremos más en detalle estos aspectos cuando hablemos de arquitecturas de datos.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sistemas de gestión de datos</span>"
    ]
  },
  {
    "objectID": "content/sistemas/rdbms_transaccional.html",
    "href": "content/sistemas/rdbms_transaccional.html",
    "title": "7  Base de datos operacional",
    "section": "",
    "text": "7.1 Transacciones\nPara entender las capacidades de los sistemas empleados en tiempo de aplicación, debemos entender primero el concepto de las transacciones.\nCuando realizamos operaciones sobre una base de datos que presenta la información en estructuras normalizadas, casi de forma segura deberemos realizar más de una operación sobre estas estructuras. Esto obliga a que si una de esas acciones produjera un error debamos considerar todo el paquete de operaciones nulo. Pensad en el ejercicio de hacer una trasferencia bancaria.\n¿Y si pasara algo en medio? Un corte, un fallo… ¿Puede ser que el dinero desaparezca del sistema? En lo que a datos se refiere podría suceder pero en la realidad sabemos que el dinero presenta un concepto de que no se volatiliza de tal modo. Es decir, que habiendo identificado el fallo en la segunda operación, deberíamos invalidar la primera y hacer que el balance se muestre como al inicio. Para ello, podemos enviar nuestras operaciones como de un paquete se tratara y la base de datos se encarga de que así suceda.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base de datos operacional</span>"
    ]
  },
  {
    "objectID": "content/sistemas/rdbms_transaccional.html#transacciones",
    "href": "content/sistemas/rdbms_transaccional.html#transacciones",
    "title": "7  Base de datos operacional",
    "section": "",
    "text": "El dinero sale de la cuenta origen\nEl dinero se inserta en la cuenta destinataria\n\n\n&lt;transacciones&gt;\n    &lt;operación&gt;El dinero sale de la cuenta origen&lt;/operación&gt;\n    &lt;operación&gt;El dinero se inserta en la cuenta destinataria&lt;/operación&gt;\n&lt;transacciones/&gt;\n\n7.1.1 ACID\nA este paquete podemos llamarlo transacción y los sistemas capaces de gestionar estos paquetes los conocemos con el nombre de sistemas transaccionales. Estos sistemas suelen además cubrir una serie de requisitos que se conocen bajo el acrónimo ACID:\n\nAtomicidad: La Atomicidad asegura que una transacción se complete de forma completa o no se realice en absoluto, es decir, todos los pasos deben ejecutarse correctamente o se revierten todos\nConsistencia: La Consistencia garantiza que una transacción mantenga la base de datos en un estado válido, cumpliendo con las reglas de integridad definidas.\nIndependencia: El aislamiento de operaciones asegura que las transacciones concurrentes se ejecuten de forma independiente, evitando que se afecten mutuamente, lo que previene problemas como la doble reserva de un asiento\nDurable: La Durabilidad garantiza que una vez que una transacción se ha confirmado, sus cambios persistirán incluso en caso de fallos del sistema, almacenándose de forma permanente en disco.\n\nCon esos mínimos, podemos montar un sistema bien robusto que garantice que la actuación digital se realiza conforme a cómo precisamos. Sin embargo, ¿es necesario que nuestros sistemas cumplan con estos requisitos siempre? No, en muchas empresas, precisamente para no afectar a la operativa de estos sistemas pero poder interrogarlos con dudas de carácter analítico e integrar información de otras fuentes (externas o internas) se disponen de plataformas secundarias que no precisan de estas restricciones ya que no hay sistemas críticos que operen sobre estas. Solo se usan con un carácter analítico.\nEl hecho de cambiar el enfoque nos permite prescindir de algunas restricciones relativas a las relaciones y otras restricciones de los sistemas transaccionales que no es necesario estén presenten en los sistemas analíticos, cambiando por completo el enfoque del sistema.\n\n\n7.1.2 Usos\nPensemos que necesitamos plantear una web, un sistema con capas frontales de presentación de datos, servicios back-end de gestión de lógicas de negocio y un sistema de almacenamiento de datos.\n\n\n\n\n\nflowchart LR\n    df[(Base de datos)]\n    bend[Lógica de negocio]\n    fend[Capa visual]\n\n    df --- bend\n    bend --- fend\n\n\n\n\n\n\nEn los últimos años, la gestión de la capa transaccional ha evolucionado significativamente. Tradicionalmente, los sistemas de bases de datos como DB2 o PostgreSQL se encargaban de controlar las transacciones, asegurando la integridad y coherencia de los datos mediante mecanismos internos. Sin embargo, con el auge de arquitecturas más complejas y distribuidas, esta responsabilidad ha pasado en gran medida a los frameworks de desarrollo liberando de esta carga al sistema de almacenamiento.\nFrameworks como Spring en Java, entre otros, permiten definir y gestionar transacciones directamente en la lógica de negocio, desacoplando el control transaccional del sistema de almacenamiento. Esto facilita la implementación de reglas de negocio más flexibles y adaptadas a las necesidades de cada aplicación, además de permitir la integración con múltiples fuentes de datos y servicios externos. Así, la capa transaccional se ha convertido en una parte fundamental de los frameworks modernos, proporcionando herramientas avanzadas para el manejo de la consistencia y la recuperación ante fallos.\nLa base de datos operacional es el sistema pensado para dar soporte a las aplicaciones que serán empleadas por agentes o usuarios para llevar acabo el proceso de negocio que ocupe. Por lo tanto, serán la fuente de información de donde deberemos recibir los datos para nuestros análisis posteriores. Y muy posiblemente debamos conocer algo de SQL para poder interactuar con estos sistemas.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base de datos operacional</span>"
    ]
  },
  {
    "objectID": "content/sistemas/sql.html",
    "href": "content/sistemas/sql.html",
    "title": "8  Structured Query Language",
    "section": "",
    "text": "8.1 DDL\nTambién abreviado como SQL, es el lenguaje de alto nivel que desde los años 70 del siglo XX nos permite interactuar con estructuras tabulares en sistemas de gestión de bases de datos tabulares o RDBMS1.\nExisten varios subgrupos dentro de la especificación de SQL dependiendo de las tareas que nos interesen:\nY en los casos en los que nuestro sistema permita la gestión de transacciones Transaction Control Language (TCL).\nTras la etapa de modelado y haber seleccionado nuestro repositorio de datos, emplearemos los comandos que nos permiten crear el homólogo físico de nuestro modelo en el sistema.\nEs decir, si nuestro modelo dispone una tabla con ALUMNOS en nuestro modelo\nerDiagram\n    ALUMNOS {\n        numero id_alumno PK\n        texto nombre\n        texto apellido\n    }\nTenemos que crear las instrucciones que indican esta estructura al sistema\nUan cuestión clave cuestiones en esta sentencia, la tipología de datos deberá ser establecida en base a los tipos de datos que soporte nuestro sistema. Además, podemos incluir la restricción de que el id_alumno debe ser único,\nO bien crearlo en una única sentencia.\nSi, por ejemplo, seleccionamos SQLite para nuestro ejemplo, podemos ver cómo resulta de forma práctica.\nimport sqlite3\n\n# Conectamos con la base de datos chinook.db\ncon = sqlite3.connect(\"my.db\")\n\n# Obtenemos un cursor que utilizaremos para hacer las consultas\ncur = con.cursor()\nY continuar ejecutando nuestra creación de tabla.\n# La creamos si no existe\ncur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS alumnos (\n        id_alumno INTEGER,\n        nombre TEXTO,\n        apellido TEXTO,\n        PRIMARY KEY(id_alumno)\n    )\n\"\"\");\n\n# Mostramos la información de la tabla\ncur.execute(\"PRAGMA table_info(alumnos)\").fetchall()\n\n[(0, 'id_alumno', 'INTEGER', 0, None, 1),\n (1, 'nombre', 'TEXTO', 0, None, 0),\n (2, 'apellido', 'TEXTO', 0, None, 0)]\nHemos añadido la condición IF NOT EXISTS que nos permite crear una tabla solo si no existe. Si intentara crearla existiendo previamente, nos arrojará un error.\n# La creamos si no existe\ncur.execute(\"\"\"\n    CREATE TABLE alumnos (\n        id_alumno INTEGER,\n        nombre TEXTO,\n        apellido TEXTO,\n        PRIMARY KEY(id_alumno)\n    )\n\"\"\");\n\n\n---------------------------------------------------------------------------\nOperationalError                          Traceback (most recent call last)\nCell In[4], line 2\n      1 # La creamos si no existe\n----&gt; 2 cur.execute(\"\"\"\n      3     CREATE TABLE alumnos (\n      4         id_alumno INTEGER,\n      5         nombre TEXTO,\n      6         apellido TEXTO,\n      7         PRIMARY KEY(id_alumno)\n      8     )\n      9 \"\"\");\n\nOperationalError: table alumnos already exists\nLa función .fetchall() tras la ejecución del PRAGMA se realice para que retorne la información. Un cursor ejecuta las acciones en el sistema destino, la base de datos, pero para obtener de vuelta el resultado de la operación, deberemos pedirlo expresamente.\nPodemos seguir extendiendo nuestro modelo, con el resto de estructuras, donde deberemos indicar los campos que registran claves foráneas, para que exista un nexo explícito entre ambas tablas. Esto permite, por ejemplo, que no de de alta a un alumno que no esté en el listado de alumnos previamente.\nerDiagram\n    ALUMNOS {\n        numero id_alumno PK\n        texto nombre\n        texto apellido\n    }\n    CURSA{\n        numero id_alumno FK\n        numero id_asignatura FK\n        numero id_profesor FK\n        numero anio\n    }\n    ASIGNATURAS {\n        numero id_asignatura PK\n        texto nombre\n    }\n    PROFESORES {\n        numero id_profesor PK\n        texto nombre\n    }\n\n    ALUMNOS }|--|{ CURSA: matriculado\n    CURSA }|--|{ ASIGNATURAS: cursando\n    PROFESORES ||--o{ CURSA: imparte\n# La creamos si no existe\ncur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS asignaturas (\n        id_asignatura INTEGER PRIMARY KEY,\n        nombre TEXTO\n    )\n\"\"\");\n\ncur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS cursa (\n        id_alumno INTEGER,\n        id_asignatura INTEGER,\n        FOREIGN KEY (id_alumno) REFERENCES alumnos (id_alumno),\n        FOREIGN KEY (id_asignatura) REFERENCES asignaturas (id_asignatura)\n    )\n\"\"\");\n\n# Mostramos la información de la tabla\ncur.execute(\"PRAGMA table_info(cursa)\").fetchall()\n\n[(0, 'id_alumno', 'INTEGER', 0, None, 0),\n (1, 'id_asignatura', 'INTEGER', 0, None, 0)]\nCon esto hemos informado de una estructura base que ahora podremos poblar con datos.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Structured Query Language</span>"
    ]
  },
  {
    "objectID": "content/sistemas/sql.html#ddl",
    "href": "content/sistemas/sql.html#ddl",
    "title": "8  Structured Query Language",
    "section": "",
    "text": "CREATE TABLE alumnos (\n    id_alumno INTEGER,\n    nombre TEXT,\n    apellido TEXT,\n)\n\nALTER TABLE alumnos ADD PRIMARY KEY(id_alumno)\n\nCREATE TABLE alumnos (\n    id_alumno INTEGER,\n    nombre TEXT,\n    apellido TEXT,\n    PRIMARY KEY (id_alumno)\n)\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n\n\nPuede que necesitéis ejecutar\ncur.execute(\"\"\"PRAGMA foreign_keys = ON;\"\"\");\npara poder contemplar las restricciones de clave foránea que veremos más adelante.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Structured Query Language</span>"
    ]
  },
  {
    "objectID": "content/sistemas/sql.html#dml",
    "href": "content/sistemas/sql.html#dml",
    "title": "8  Structured Query Language",
    "section": "8.2 DML",
    "text": "8.2 DML\nEl lenguaje de manipulación de datos nos permite introducir información, alterarla, borrar y por último seleccionar o extraer información del sistema.\n\n8.2.1 Inserciones\nInsertamos datos de un alumno.\n\ncur.execute(\"\"\"\n    INSERT INTO alumnos VALUES (1, 'Iraitz', 'Montalban');\n\"\"\");\n\n\ncur.execute(\"\"\"\n    INSERT INTO alumnos VALUES (1, 'Iraitz', 'Montalban');\n\"\"\");\n\n\n---------------------------------------------------------------------------\nIntegrityError                            Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 cur.execute(\"\"\"\n      2     INSERT INTO alumnos VALUES (1, 'Iraitz', 'Montalban');\n      3 \"\"\");\n\nIntegrityError: UNIQUE constraint failed: alumnos.id_alumno\n\n\n\nEstas restricciones se extienden a las tablas con clave foránea, ya que podríamos estar matriculando a un alumno que no existe en una asignatura que no existe.\n\ncur.execute(\"\"\"\n    INSERT INTO cursa VALUES (2, 1);\n\"\"\");\n\n\n---------------------------------------------------------------------------\nIntegrityError                            Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 cur.execute(\"\"\"\n      2     INSERT INTO cursa VALUES (2, 1);\n      3 \"\"\");\n\nIntegrityError: FOREIGN KEY constraint failed\n\n\n\nEs una de las labores clave de los sistemas transaccionales, garantizar que el sistema responde a las lógicas de los procesos de negocio que hemos modelado.\n\n\n8.2.2 Actualizaciones\nPodemos actualizar su información, donde la cláusula where permite filtrar y aislar qué fila deberemos actualizar.\n\ncur.execute(\"\"\"\n    UPDATE alumnos SET apellido = 'Montalbán' WHERE id_alumno = 1;\n\"\"\");\n\n\n\n8.2.3 Selección\nY extraer información de estas tablas, recordando que si traemos información de la fuente deberemos hacer un fetch para que la retorne a nuestro cliente.\n\ncur.execute(\"\"\"\n    SELECT * \n    FROM alumnos\n\"\"\").fetchall()\n\n[(1, 'Iraitz', 'Montalbán')]\n\n\n\n\n8.2.4 Desnormalización\nSi contamos con información asociada deberemos juntarla para poder extraer los datos relevantes.\n\n# Nuevos alumnos\ncur.execute(\"\"\"\n    INSERT INTO alumnos VALUES (2, 'Javier', 'Garcia'), (3, 'Miguel', 'Fernandez');\n\"\"\");\n\n# Nuevas asignaturas\ncur.execute(\"\"\"\n    INSERT INTO asignaturas VALUES (1, 'Matemáticas'), (2, 'Historia'), (3, 'Biología');\n\"\"\");\n\n# Matrículas\ncur.execute(\"\"\"\n    INSERT INTO cursa VALUES (1, 2), (2, 3), (3, 3);\n\"\"\");\n\nPor ejemplo, ¿qué alumnos cursan biología? No podemos saberlo de la tabla que almacena la relación.\n\ncur.execute(\"\"\"\n    SELECT * \n    FROM cursa\n\"\"\").fetchall()\n\n[(1, 2), (2, 3), (3, 3)]\n\n\nDeberemos juntarla con la tabla que presenta la información de las asignaturas y filtrar por nombre.\n\ncur.execute(\"\"\"\n    SELECT * \n    FROM cursa c\n        JOIN asignaturas asig ON asig.id_asignatura = c.id_asignatura\n    WHERE asig.nombre = 'Biología'\n\"\"\").fetchall()\n\n[(2, 3, 3, 'Biología'), (3, 3, 3, 'Biología')]\n\n\nY esta a su vez juntarla con los alumnos para obtener los datos de estos.\n\ncur.execute(\"\"\"\n    SELECT * \n    FROM cursa c\n        JOIN asignaturas asig ON asig.id_asignatura = c.id_asignatura\n        JOIN alumnos alum ON alum.id_alumno = c.id_alumno\n    WHERE asig.nombre = 'Biología'\n\"\"\").fetchall()\n\n[(2, 3, 3, 'Biología', 2, 'Javier', 'Garcia'),\n (3, 3, 3, 'Biología', 3, 'Miguel', 'Fernandez')]\n\n\nGracias a la estructura tabular tenemos las relaciones y los datos de nuestras entidades bien representados una vez que desnormalizamos y bien podemos seleccionar los atributos (columnas) que sean de interés.\n\ncur.execute(\"\"\"\n    SELECT alum.nombre, alum.apellido\n    FROM cursa c\n        JOIN asignaturas asig ON asig.id_asignatura = c.id_asignatura\n        JOIN alumnos alum ON alum.id_alumno = c.id_alumno\n    WHERE asig.nombre = 'Biología'\n\"\"\").fetchall()\n\n[('Javier', 'Garcia'), ('Miguel', 'Fernandez')]\n\n\nO incluso realizar agregaciones sobre estos datos.\n\ncur.execute(\"\"\"\n    SELECT asig.nombre, COUNT(alum.id_alumno) AS cuantos_alumnos\n    FROM cursa c\n        JOIN asignaturas asig ON asig.id_asignatura = c.id_asignatura\n        JOIN alumnos alum ON alum.id_alumno = c.id_alumno\n    WHERE asig.nombre = 'Biología'\n    GROUP BY asig.nombre\n\"\"\").fetchall()\n\n[('Biología', 2)]",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Structured Query Language</span>"
    ]
  },
  {
    "objectID": "content/sistemas/sql.html#tcl",
    "href": "content/sistemas/sql.html#tcl",
    "title": "8  Structured Query Language",
    "section": "8.3 TCL",
    "text": "8.3 TCL\nLas transacciones nos obligan a declarar procesos en la propia base de datos de forma que podamos invocarlos como si de funciones se trataran. Esto permite a la base de datos realizar su labor y gestionar todas las operaciones de las transacciones tal y como se precisan.\n\nBEGIN TRANSACTION;\n\nINSERT INTO alumnos VALUES (4, 'María', 'Garcia');\n\nINSERT INTO cursa VALUES (4, 2), (4, 1);\n\nCOMMIT;\nEl COMMIT final se hace cargo de realizar toda la operación y en caso de fallo siempre podremos recurrir al ROLLBACK que deshace los cambios realizados al inicio de la transacción sin afectar a otros procesos del sistema.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Structured Query Language</span>"
    ]
  },
  {
    "objectID": "content/sistemas/sql.html#footnotes",
    "href": "content/sistemas/sql.html#footnotes",
    "title": "8  Structured Query Language",
    "section": "",
    "text": "Os dejo una muy buena entrada sobre la historia de SQL↩︎",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Structured Query Language</span>"
    ]
  },
  {
    "objectID": "content/sistemas/dbms_analítica.html",
    "href": "content/sistemas/dbms_analítica.html",
    "title": "9  Base de datos analítica",
    "section": "",
    "text": "9.1 Ordenación de los datos\nSi bien es cierto que heredamos de los sistemas operacionales la estructura base, las necesidades de un sistema analítico son muy distintos.\nLas bases de datos operacionales para facilitar el acceso concurrente a los datos, ordenan estos en filas.\nEn los ficheros convencionales ordenados por filas la información la representaríamos así.\nMientras que las consultas analíticas precisan acceder a la información en columnas. Por ejemplo, si quisiéramos hacer un promedio de la edad de nuestros alumnos, solo necesitamos esa cuarta columna. Es decir, sabiendo qué número de filas tenemos solo debemos posicionarnos donde empieza la cuarta columna y leer los siguientes 2 datos, obviando todo lo anterior.\nDe forma que la ordenación columnar nos permite no tener que cargar la información innecesaria en la operación AVG(edad).\nEn el mundo de los sistemas de código abierto, Apache Parquet para el almacenamiento en disco y Apache Arrow para el mismo concepto en memoria se han convertido en un estándar.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Base de datos analítica</span>"
    ]
  },
  {
    "objectID": "content/sistemas/dbms_analítica.html#ordenación-de-los-datos",
    "href": "content/sistemas/dbms_analítica.html#ordenación-de-los-datos",
    "title": "9  Base de datos analítica",
    "section": "",
    "text": "id\nnombre\napellido\nedad\n\n\n\n\n1\nIraitz\nMontalbán\n18\n\n\n2\nJavier\nGarcia\n19\n\n\n\n\n\n1;Iraitz;Montalbán;18;2;Javier;Garcia;19\n\n\n\n1;2;Iraitz;Javier;Montalbán;Garcia;18;19",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Base de datos analítica</span>"
    ]
  },
  {
    "objectID": "content/sistemas/dbms_analítica.html#restricciones",
    "href": "content/sistemas/dbms_analítica.html#restricciones",
    "title": "9  Base de datos analítica",
    "section": "9.2 Restricciones",
    "text": "9.2 Restricciones\nPara el caso general no existen cuestiones como las restricciones de clave primaria y clave foránea, ya que el sistema origen es quien impone estas restricciones y el sistema analítico recoge la información tal y como este la provea.\nDel mismo modo, de cara a hacer las operaciones más rápidas, los índices son clave en los sistemas operacionales. Las bases de datos analíticas no presentan esta necesidad ya que solemos consumir la información en bloques y se asumen las latencias de estos procesos por no impacta a procesos de negocio. Si que tener una buena ordenación nos ayuda a no leer información que no vayamos a utilizar y aligerar la carga del proceso, de ahí que si sea común tener políticas de particionado.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Base de datos analítica</span>"
    ]
  },
  {
    "objectID": "content/sistemas/dbms_analítica.html#sistemas-comunes",
    "href": "content/sistemas/dbms_analítica.html#sistemas-comunes",
    "title": "9  Base de datos analítica",
    "section": "9.3 Sistemas comunes",
    "text": "9.3 Sistemas comunes\n\n9.3.1 Nube\n\nAzure Synpase SQL\nRedshift\nGoogle BigQuery\n\n\n\n9.3.2 Multinube\n\nSnowflake\nDatabricks\nMotherduck\n\n\n\n9.3.3 On-premise\n\nMicrosoft SQL Server\nApache Hadoop\nClickhouse\nDuckDB\nApache Doris\n\n\n\n9.3.4 Motores de consulta\n\nTrino\nPinot\nApache Spark\nStardust\nDaft",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Base de datos analítica</span>"
    ]
  },
  {
    "objectID": "content/sistemas/arquitecturas.html",
    "href": "content/sistemas/arquitecturas.html",
    "title": "10  Elección de arquitecturas",
    "section": "",
    "text": "10.1 Top-down o bottom-up\nEs una fuente constante de debate pensar en si existe una mejor manera de hacer lo que hacemos, y cómo movemos los datos de un extremo al otro. Y siempre habrá nuevas piezas técnicas a descubrir que nos permitirán hacer las cosas mejor, pero la base, el servicio que ofrecemos a los consumidores de datos y la lógica tras nuestro modelo de datos debe perdurar ya que es un fiel reflejo de nuestra organización y esto no cambia tan frecuentemente. Ya lo decía Conway\nQuizás con un foco más operativo debido a los sistemas existentes, tanto Bill Inmon como Ralph Kimball se enfrentaron al reto de almacenar de forma efectiva toda la información que contiene una organización, pero a su vez presentar vistas específicas y cohesionadas que permitieran a los usuarios entender lo que los datos reflejaban sobre el funcionamiento de la empresa en áreas concretas.\nEl enfoque de Inmon abogaba por fijarnos en las fuentes y generar un sistema centralizado de datos al que luego poder destilar los recursos necesarios.\nflowchart LR\n    source1[\"ERP empresarial\"]\n    source2[\"Base de datos Logistica\"]\n\n    etl1[\"ETL\"]\n\n    dw[(\"Sistema informacional\")]\n\n    etl2[\"ETL\"]\n\n    datamart1[\"Data Mart Finanzas\"]\n    datamart2[\"Data Mart Operaciones\"]\n\n    source1 --- etl1\n    source2 --- etl1\n\n    etl1 --- dw\n\n    dw --- etl2\n\n    etl2 --- datamart1\n    etl2 --- datamart2\nMientras que la colección de Data Marts representan para Kimball el Data Warehouse, la pieza central. Desde las unidades de negocio podemos indicar las dimensiones y hechos a agregar que deberemos materializar en visiones concretas para cada unidad donde, entendemos, las dimensiones serán comunes dado que trabajamos en unidades de una empresa en una misma industria.\nflowchart LR\n    source1[\"ERP empresarial\"]\n    source2[\"Base de datos Logistica\"]\n\n    etl1[\"ETL\"]\n\n    subgraph dw [\"Sistema informacional\"]\n        datamart1[\"Data Mart Finanzas\"]\n        datamart2[\"Data Mart Operaciones\"]\n    end\n\n    source1 --- etl1\n    source2 --- etl1\n\n    etl1 --- dw\nEstas dos aproximaciones han sentado la base de lo que es necesario en un warehouse, tanto para el equipo que lo mantiene y quieren dar un buen servicio como paras los consumidores de datos que los necesitan para tener una operativa inteligente.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elección de arquitecturas</span>"
    ]
  },
  {
    "objectID": "content/sistemas/arquitecturas.html#la-caja-fuerte",
    "href": "content/sistemas/arquitecturas.html#la-caja-fuerte",
    "title": "10  Elección de arquitecturas",
    "section": "10.2 La caja fuerte",
    "text": "10.2 La caja fuerte\nEl Data Vault, se plateó como una metodología más allá del propio sistema de modelado intermedio. Habíamos aprendido mucho sobre cómo nos comunicamos con los negocios y sus necesidades. Pero esto también nos daba una perspectiva de cómo la información debía ser promocionada hacia las capas de consumo.\n\n\n\nFases de madurez del dato en DV\n\n\nSabemos que debemos recibir información cruda de las fuentes, y esta información debe reposar al menos por un tiempo para poder ser entendida y gestionada. En el mejor caso, simplemente se integrará con las estructuras existentes. En el peor, deberemos alterar nuestro modelo para albergar estos nuevos datos e informar de su existencia a capas superiores. Es el área de aterrizaje de los datos o staging.\nUna vez entendidos los datos y su cohesión en los conceptos de negocio, formarán parte de la estructura base de nuestro sistema informacional. Quizás podamos aplicar algunas restricciones de negocio o validar la bondad de los datos recibidos. Todas estas fases son parte de una etapa intermedia que en el caso de data vault, forman el raw vault, con la información cruda ya estructurada; y el business vault donde ciertas reglas de negocio puedan ser aplicadas.\nFinalmente, el usuario debe ser abstraído de toda esta complejidad técnica. Una última capa pensada para la extracción de información, en un modelado en estrella o de tabla única, es donde la información resulta sencilla de consumir y con suerte disponemos de información adicional sobre cuándo fue actualizada por última vez, si algo ha cambiado en los datos, etc. Es la capa de consumo o business intelligence.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elección de arquitecturas</span>"
    ]
  },
  {
    "objectID": "content/sistemas/arquitecturas.html#la-era-big-data",
    "href": "content/sistemas/arquitecturas.html#la-era-big-data",
    "title": "10  Elección de arquitecturas",
    "section": "10.3 La era Big Data",
    "text": "10.3 La era Big Data\nPara principios de los 2000s, varias empresas se enfrentaban a problemas relativos a la velocidad y variabilidad de los datos que venían de procedencias ajenas a la red empresarial: internet. El volumen 1 que debían de procesar bien al vuelo como en reposo, crecía exponencialmente. Y si añadimos a esto la necesidad de modelar al mismo ritmo debido a la variabilidad de los formatos (XML en gran medida en aquella época). Recordemos que los sistemas tabulares requerían de informar del esquema de la tabla incluyendo el tipo de dato de antemano, y cada cambio supone un quebradero de cabeza de gestión.\nYa en 1998, Carlo Strozzi había sugerido el uso de sistemas que no solo soportarán SQL. Es decir, que permitieran lo que conocemos como schema-on-read trasladando el problema de modelado a capas posteriores del sistema de forma que al menos, no perdiéramos información. En la misma medida, los equipos de Google y Yahoo! se encontraban en su pelea particular de cómo almacenar y procesar la información disponible en internet de manera económica y efectiva.\nUn par de trabajos del equipo de Google (Google File System y Map Reduce) motivaron la implementación de Mike Cafarella y Doug Cutting, Apache Hadoop un sistema que tuvo una adopción vertiginosa en la década de 2010, algo antes de que se desencadenara el movimiento a la nube.\nEste sistema fue de los primeros que ofrecía a las empresas un sistema desacoplado entre almacenamiento y computación, con un sistema de código abierto, lo cual lo hacía accesible a todo el mundo al coste de ser uno mismo quien mantiene este sistema organizado.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elección de arquitecturas</span>"
    ]
  },
  {
    "objectID": "content/sistemas/arquitecturas.html#la-casa-del-lago",
    "href": "content/sistemas/arquitecturas.html#la-casa-del-lago",
    "title": "10  Elección de arquitecturas",
    "section": "10.4 La casa del lago",
    "text": "10.4 La casa del lago\nHadoop estaba basado en un sistema de ficheros, lo cual daba la versatilidad de almacenar lo que fuera necesario, sin embargo, consumir datos de esta fuente se volvía engorroso. Dado que los conocimientos de los equipos seguían centrándose en SQL y herramientas que podían realizar este tipo de consultas, los ingenieros de Facebook tomaron como referencia el trabajo del equipo de Google Dremel y crearon un sistema que permitía consultar información tabular en el sistema, Apache Hive. Simplemente catalogando cómo realizar la traducción tabla a fichero, estos sistemas abstraían a los usuarios de las complejidades del lago de datos empezando a vislumbrar lo que años más tarde el equipo de Databricks denominaría la casa del lago (o Data Lakehouse).",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elección de arquitecturas</span>"
    ]
  },
  {
    "objectID": "content/sistemas/arquitecturas.html#ordenando-el-lago",
    "href": "content/sistemas/arquitecturas.html#ordenando-el-lago",
    "title": "10  Elección de arquitecturas",
    "section": "10.5 Ordenando el lago",
    "text": "10.5 Ordenando el lago\nTras la polvareda generada por la tumultuosa era de los clústeres Hadoop, la nube se abrió camino y con ello soluciones que haciendo uso de las tecnologías abiertas disponibles ofrecieron soluciones renovadas a las empresas. En cierta forma es como si hubiéramos reinventado los sistemas de gestión de base de datos cayendo en los mismos problemas que presentaban los sistemas originales. No es que no hubiera almacenamiento columnar antes de Apache Parquet o computación distribuida en memoria antes de Apache Spark, pero a veces tenemos que romper el status quo para apreciar el trabajo existente.\nDos grandes empresas salieron de esta época, Snowflake con su sistema de gestión de bases de datos tabulares que permitía una gestión desacoplada de almacenamiento y computo. Y Databricks, cuyo co-fundador es Matei Zaharia, autor principal del trabajo en Apache Spark2.\nEstas dos soluciones de algún modo han vuelto a enfocarse en ofrecer rendimiento y estructura a los sistemas de datos. Han incorporado necesidades como las de lanzar procesos de bajo nivel o permitir flexibilizar los tipos de datos (VARIANT o JSON) para albergar estructuras anidadas de datos\n{\n    \"persona\": {\n        \"nombre\": \"Iraitz\",\n        \"appelido\": \"Montalban\",\n        \"empresas\": \n            [\n                \"SDG Group\"\n                \"Iberdrola\",\n                \"panda Security\",\n                \"Tecnalia\"\n            ]\n        }\n    }\n}\npero se centran en disponer la información de forma tabular a sus consumidores. Databricks se ha encargado de acuñar varios términos como el Lakehouse para este tipo de flexibilización de la base de datos tradicional que incluye además capacidades operacionales; o incluso la arquitectura medallón3\n\n\n\nArquitectura medallón\n\n\nQue de algún modo, reinventa las tres capas base que ya veníamos empleando con otro nombre. Podríamos llamarlo la arquitectura del sentido común ya que sabemos que nuestras fuentes y nuestros consumidores vana tener distintas necesidades y únicamente nos estamos concediendo un hueco entre ambos para poder realizar nuestro trabajo y consolidar toda la información de la empresa de forma que sea manejable.",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elección de arquitecturas</span>"
    ]
  },
  {
    "objectID": "content/sistemas/arquitecturas.html#footnotes",
    "href": "content/sistemas/arquitecturas.html#footnotes",
    "title": "10  Elección de arquitecturas",
    "section": "",
    "text": "Con esta tercera V es como conocemos las tres Vs del Big Data.↩︎\nJunto con nombres como Ion Stoica↩︎\nDe su fuente oficial https://www.databricks.com/glossary/medallion-architecture↩︎",
    "crumbs": [
      "Sistemas gestores de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elección de arquitecturas</span>"
    ]
  },
  {
    "objectID": "content/load/intro.html",
    "href": "content/load/intro.html",
    "title": "11  Extracción",
    "section": "",
    "text": "11.1 Bases de datos\nUno de los pasos primeros que deberemos hacer es conocer los datos a extraer y las características base de estos:\nDe modo que podamos plantear cómo esta información será recibida en el sistema destino y si es necesario realizar algún paso intermedio.\nFrecuentemente son las fuentes donde reside la información en una organización. Suelen adherirse a protocolos concretos como y devolver la información en una estructura similar a la de gestión interna. Existen dos grandes familias:\nMuchas de las bases de datos de nueva generación también exponen APIs de forma que podemos realizar las consultas siguiendo un protocolo HTTP de comunicación.",
    "crumbs": [
      "Ingesta de datos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Extracción</span>"
    ]
  },
  {
    "objectID": "content/load/intro.html#bases-de-datos",
    "href": "content/load/intro.html#bases-de-datos",
    "title": "11  Extracción",
    "section": "",
    "text": "SQL: Sistemas de origen tabular (RDBMS) a los que podemos interrogar con sentencias SQL para extraer la información necesaria. Son sistemas de consulta por lo que a no ser que dispongan de un campo informando de la fecha de la información, es difícil gestionar posibles variaciones con respecto a las cargas anteriores sin almacenar en un sistema intermedio la información para comparar que fue cargado y qué es nuevo.\nNoSQL: Sistemas de origen no tabular que suelen disponer la información en formatos modernos, siendo JSON el más habitual. Esto obliga a identificar si el destino permite el registro de información de estructura variable o debemos darle un formato tabular antes de poder insertarla. Al igual que con las bases de datos tabulares, deberemos ver qué medios tenemos para poder identificar los registros sumados o que hayan variado desde la última consulta.",
    "crumbs": [
      "Ingesta de datos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Extracción</span>"
    ]
  },
  {
    "objectID": "content/load/intro.html#apis",
    "href": "content/load/intro.html#apis",
    "title": "11  Extracción",
    "section": "11.2 APIs",
    "text": "11.2 APIs\nUno de los mecanismos más efectivos a la hora de interconectar sistemas remotos. Aunque originalmente van más orientados a disponer de un medio por el que invocar vía HTTP alguna acción qu bien pudiera ser una de nuestras operaciones CRUD (insertar, cambiar, borrar o mostrar), la capacidad de consulta hace que sea un recurso de consumo de activos de datos muy usual.",
    "crumbs": [
      "Ingesta de datos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Extracción</span>"
    ]
  },
  {
    "objectID": "content/load/intro.html#colas",
    "href": "content/load/intro.html#colas",
    "title": "11  Extracción",
    "section": "11.3 Colas",
    "text": "11.3 Colas\nLos sistemas de mensajería son un canal ideal cuando tenemos información en vuelo, ya que de manera natural sirven los datos que no hayan sido consumidos. Esos sistemas suelen tener mecanismos de buffer, con lo que la información expira pasado un tiempo pero nos permiten interconectar sistemas de generación constante como sensores o medios IoT con sistemas de cadencia de consulta menos frecuente. Además, nos permiten disponer de una arquitectura desacoplada de forma que cambios tanto en sistema origen como en el medio de consumo de datos pueden hacerse contando con este buffer que retiene los datos hasta ser consumidos o expiren.",
    "crumbs": [
      "Ingesta de datos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Extracción</span>"
    ]
  },
  {
    "objectID": "content/transform/intro.html",
    "href": "content/transform/intro.html",
    "title": "12  Transformación",
    "section": "",
    "text": "pendiente",
    "crumbs": [
      "Transformación de datos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformación</span>"
    ]
  },
  {
    "objectID": "content/explotacion/intro.html",
    "href": "content/explotacion/intro.html",
    "title": "Explotación de los datos",
    "section": "",
    "text": "pendiente",
    "crumbs": [
      "Explotación de los datos"
    ]
  },
  {
    "objectID": "content/explotacion/bi.html",
    "href": "content/explotacion/bi.html",
    "title": "13  Inteligencia de negocio",
    "section": "",
    "text": "La inteligencia de negocio o Business Intelligence (BI) es sin duda uno de los recursos clave a la hora de conocer la actividad de nuestra empresa. Toda esa información que los datos ocultan, si la manipulamos de forma correcta y la presentamos de un modo lógico puede ayudarnos a determinar si estamos comprando de más, vendiendo de menos, si nuestras líneas de fabricación están siendo bien empleadas o si nuestros almacenes requieren una renovación del stock.",
    "crumbs": [
      "Explotación de los datos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inteligencia de negocio</span>"
    ]
  },
  {
    "objectID": "content/explotacion/ds.html",
    "href": "content/explotacion/ds.html",
    "title": "14  Ciencia de datos",
    "section": "",
    "text": "pendiente",
    "crumbs": [
      "Explotación de los datos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ciencia de datos</span>"
    ]
  },
  {
    "objectID": "content/references.html",
    "href": "content/references.html",
    "title": "Referencias",
    "section": "",
    "text": "Codd, Edgar F. 1970. “A Relational Model of Data for Large Shared\nData Banks.” Communications of the ACM 13 (6): 377–87.",
    "crumbs": [
      "Referencias"
    ]
  },
  {
    "objectID": "content/appendices/de_storybook.html",
    "href": "content/appendices/de_storybook.html",
    "title": "Apéndice A — La danza del Ingeniero de Datos",
    "section": "",
    "text": "Download PDF file.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>La danza del Ingeniero de Datos</span>"
    ]
  }
]